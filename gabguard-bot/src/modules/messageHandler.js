const { analyzeTextApi, analyzeFileApi } = require('./apiService')
const { notifyAdmins, warnUser } = require('./utils')
const { handleAdminCommand } = require('./adminCommands')

async function handleTextMessage(message, config) {
  const userId = message.author.id
  try {
    const apiResponseData = await analyzeTextApi(message.content, userId, config.API_BASE_URL)
    const analyzedUserId = String(apiResponseData.user_id || userId)
    const analyzedText = String(apiResponseData.text || message.content)
    const toxicityScore = Number(apiResponseData.toxicity_score)

    console.log(`ðŸ” Wynik analizy tekstu (ID: ${analyzedUserId}): toxicity_score = ${toxicityScore}`)

    if (isNaN(toxicityScore)) {
      console.error(`ðŸš« Otrzymano nieprawidÅ‚owÄ… wartoÅ›Ä‡ toxicity_score: ${apiResponseData.toxicity_score}`)
      return
    }

    if (toxicityScore === -1) {
      console.error(`ðŸš« BÅ‚Ä…d analizy tekstu przez API dla uÅ¼ytkownika ${analyzedUserId}.`)
    } else if (toxicityScore >= config.TOXICITY_THRESHOLD_DELETE) {
      console.warn(`ðŸ—‘ï¸ Wysoka toksycznoÅ›Ä‡ (${toxicityScore})! Usuwanie wiadomoÅ›ci od ${message.author.tag}.`)
      if (message.deletable) await message.delete().catch((err) => console.error('BÅ‚Ä…d usuwania wiadomoÅ›ci:', err))
      try {
        await message.author.send(
          `Twoja wiadomoÅ›Ä‡ na serwerze "${
            message.guild.name
          }" zostaÅ‚a usuniÄ™ta z powodu wysokiego wskaÅºnika toksycznoÅ›ci (${toxicityScore.toFixed(
            2
          )}).\nTreÅ›Ä‡: "${analyzedText}"`
        )
      } catch (dmError) {
        console.warn(`Nie udaÅ‚o siÄ™ wysÅ‚aÄ‡ DM do ${message.author.tag}: ${dmError.message}`)
      }
      await notifyAdmins(
        `ðŸ—‘ï¸ UsuniÄ™to wiadomoÅ›Ä‡ od uÅ¼ytkownika @${
          message.author.tag
        } (ID: ${userId}) z powodu wysokiej toksycznoÅ›ci (${toxicityScore.toFixed(2)}).\nTreÅ›Ä‡: "${analyzedText.slice(
          0,
          100
        )}"\nKanaÅ‚: #${message.channel.name}`,
        config
      )
      await warnUser(
        session.userId,
        `Twoja wiadomoÅ›Ä‡ "${analyzedText.slice(
          0,
          100
        )}" zostaÅ‚a oznaczona jako toksyczna i usuniÄ™ta (${toxicityScore.toFixed(
          2
        )}). Prosimy o zachowanie kultury wypowiedzi.`
      )
    } else if (toxicityScore >= config.TOXICITY_THRESHOLD_WARN) {
      console.info(`âš ï¸ WiadomoÅ›Ä‡ od ${message.author.tag} oznaczona jako potencjalnie toksyczna (${toxicityScore}).`)
      await message.reply(
        `âš ï¸ Twoja wiadomoÅ›Ä‡ moÅ¼e zawieraÄ‡ nieodpowiednie treÅ›ci (wskaÅºnik toksycznoÅ›ci: ${toxicityScore.toFixed(
          2
        )}). Prosimy o zachowanie kultury wypowiedzi.`
      )
      await notifyAdmins(
        `âš ï¸ WiadomoÅ›Ä‡ od ${
          message.author.tag
        } (ID: ${userId}) oznaczona jako potencjalnie toksyczna (${toxicityScore.toFixed(
          2
        )}).\nTreÅ›Ä‡: "${analyzedText}"\nKanaÅ‚: #${message.channel.name}`,
        config
      )
    }
  } catch (error) {
    console.error(`ðŸ’¥ BÅ‚Ä…d podczas analizy tekstu dla uÅ¼ytkownika ${userId}:`, error.message)
    if (error.response) console.error('OdpowiedÅº API (text):', error.response.status, error.response.data)
  }
}

async function handleFileAttachment(attachment, message, config) {
  const userId = message.author.id
  try {
    const fileApiResponseData = await analyzeFileApi(attachment, userId, config.API_BASE_URL, config.TEMP_FILE_DIR)
    const fileAnalyzedUserId = String(fileApiResponseData.user_id || userId)
    const toxicityScore = Number(fileApiResponseData.toxicity_score)
    const description = fileApiResponseData.description ? String(fileApiResponseData.description) : null

    console.log(
      `ðŸ“Š Wynik analizy pliku ${attachment.name} (ID: ${fileAnalyzedUserId}): toxicity_score = ${toxicityScore}`
    )
    if (description) console.log(`ðŸ“„ Opis: ${description}`)

    if (isNaN(toxicityScore)) {
      console.error(
        `ðŸš« Otrzymano nieprawidÅ‚owÄ… wartoÅ›Ä‡ toxicity_score dla pliku: ${fileApiResponseData.toxicity_score}`
      )
      return
    }

    const contentIdentifier = description || `plik "${attachment.name}"`

    if (toxicityScore === -1) {
      console.error(`ðŸš« BÅ‚Ä…d analizy pliku przez API dla uÅ¼ytkownika ${fileAnalyzedUserId}.`)
    } else if (toxicityScore >= config.TOXICITY_THRESHOLD_DELETE) {
      console.warn(
        `ðŸ—‘ï¸ Wysoka toksycznoÅ›Ä‡ (${toxicityScore}) w pliku! Usuwanie wiadomoÅ›ci z zaÅ‚Ä…cznikiem od ${message.author.tag}.`
      )
      if (message.deletable)
        await message.delete().catch((err) => console.error('BÅ‚Ä…d usuwania wiadomoÅ›ci z plikiem:', err))
      try {
        await message.author.send(
          `Twoja wiadomoÅ›Ä‡ z plikiem "${attachment.name}" na serwerze "${
            message.guild.name
          }" zostaÅ‚a usuniÄ™ta z powodu wysokiego wskaÅºnika toksycznoÅ›ci zawartoÅ›ci (${toxicityScore.toFixed(2)}).`
        )
      } catch (dmError) {
        console.warn(`Nie udaÅ‚o siÄ™ wysÅ‚aÄ‡ DM do ${message.author.tag} o usuniÄ™ciu pliku: ${dmError.message}`)
      }
      await notifyAdmins(
        `ðŸ—‘ï¸ UsuniÄ™to wiadomoÅ›Ä‡ z plikiem od ${
          message.author.tag
        } (ID: ${userId}) z powodu wysokiej toksycznoÅ›ci (${toxicityScore.toFixed(2)}).\nPlik: ${
          attachment.name
        }\nZidentyfikowana treÅ›Ä‡: "${contentIdentifier}"\nKanaÅ‚: #${message.channel.name}`,
        config
      )
      await warnUser(
        session.userId,
        `Twoja wiadomoÅ›Ä‡ "${attachment.name}" zostaÅ‚a oznaczona jako toksyczna i usuniÄ™ta (${toxicityScore.toFixed(
          2
        )}). Prosimy o zachowanie kultury wypowiedzi.`
      )
    } else if (toxicityScore >= config.TOXICITY_THRESHOLD_WARN) {
      console.info(
        `âš ï¸ Plik ${attachment.name} od ${message.author.tag} oznaczony jako potencjalnie toksyczny (${toxicityScore}).`
      )
      await message.reply(
        `âš ï¸ TwÃ³j plik "${
          attachment.name
        }" moÅ¼e zawieraÄ‡ nieodpowiednie treÅ›ci (wskaÅºnik toksycznoÅ›ci: ${toxicityScore.toFixed(2)}).`
      )
      await notifyAdmins(
        `âš ï¸ Plik od ${
          message.author.tag
        } (ID: ${userId}) oznaczony jako potencjalnie toksyczny (${toxicityScore.toFixed(2)}).\nPlik: ${
          attachment.name
        }\nZidentyfikowana treÅ›Ä‡: "${contentIdentifier}"\nKanaÅ‚: #${message.channel.name}`,
        config
      )
    }
  } catch (error) {
    console.error(`ðŸ’¥ BÅ‚Ä…d podczas przetwarzania pliku ${attachment.name} od uÅ¼ytkownika ${userId}:`, error.message)
    if (error.response) console.error('OdpowiedÅº API (file):', error.response.status, error.response.data)
  }
}

async function onMessageCreate(message, config) {
  if (message.author.bot) return

  const isAdminCommandHandled = await handleAdminCommand(message, config)
  if (isAdminCommandHandled) {
    return
  }

  console.log(
    `ðŸ’¬ Otrzymano wiadomoÅ›Ä‡ od ${message.author.tag} (ID: ${message.author.id}) w kanale ${message.channel.name} na serwerze ${message.guild.name}`
  )

  // Analiza tekstu wiadomoÅ›ci
  if (message.content) {
    await handleTextMessage(message, config)
  }

  // Analiza zaÅ‚Ä…cznikÃ³w (plikÃ³w)
  if (message.attachments.size > 0) {
    for (const attachment of message.attachments.values()) {
      await handleFileAttachment(attachment, message, config)
    }
  }
}

module.exports = {
  onMessageCreate,
}
